{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "16063932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "800f3c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather_data(latitude, longitude, start_date, end_date):\n",
    "    latitude = str(latitude)\n",
    "    longitude = str(longitude)\n",
    "    response = requests.get(\"https://archive-api.open-meteo.com/v1/era5?latitude=\" + latitude + \"&longitude=\" + longitude + \"&start_date=\" + start_date + \"&end_date=\" + end_date + \"&hourly=precipitation\")\n",
    "    #print(response.status_code)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        times = data[\"hourly\"][\"time\"]\n",
    "        temperatures = data[\"hourly\"][\"precipitation\"]\n",
    "        time_temperature_tuple = []\n",
    "        for i in range(0, len(times)):\n",
    "            time_temperature_tuple.append((times[i], temperatures[i]))\n",
    "        return time_temperature_tuple\n",
    "    else:\n",
    "        return \"Nah\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d63131f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = {\n",
    "    \"High Bridge\": [40.667931, -74.895576],\n",
    "    \"Annandale\": [40.640911, -74.881126],\n",
    "    \"Lebanon\": [40.641781, -74.835358],\n",
    "    \"White House\": [40.6155126, -74.7707356],\n",
    "    \"North Branch\": [40.6020469, -74.6773823],\n",
    "    \"Raritan\": [40.567181, -74.634683],\n",
    "    \"Somerville\": [40.5742696, -74.60988],\n",
    "    \"Bridgewater\": [40.5598127, -74.5517146],\n",
    "    \"Bound Brook\": [40.5684363, -74.5384889],\n",
    "    \"Dunellen\": [40.5892696, -74.4718201],\n",
    "    \"Plainfield\": [40.6337136, -74.4073737],\n",
    "    \"Netherwood\": [40.6290506, -74.4033895],\n",
    "    \"Fanwood\": [40.6409555, -74.383846],\n",
    "    \"Westfield\": [40.6589912, -74.3473717],\n",
    "    \"Garwood\": [40.6517692, -74.3229264],\n",
    "    \"Cranford\": [40.6584358, -74.2995923],\n",
    "    \"Roselle Park\": [40.6645469, -74.2643133],\n",
    "    \"Union\": [40.698967, -74.266861],\n",
    "    \"Newark Penn Station\": [40.732598, -74.174796],\n",
    "    \"Secaucus Junction\": [40.788513, -74.058787],\n",
    "    \"Penn Station New York\": [40.750079, -73.991348]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bd10475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.2, 0.2, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('2019_10.csv')\n",
    "data = data.dropna()\n",
    "data = data[data['type'] == 'NJ Transit']\n",
    "data = data[data['line'] == 'Raritan Valley']\n",
    "data = data.drop(columns=['train_id', 'from', 'from_id', 'to_id', 'line', 'date', 'stop_sequence', 'actual_time', 'type', 'status'])\n",
    "\n",
    "# Demo data\n",
    "# data = {\n",
    "#     \"delay_minutes\": [1, 2, 2, 3, 4, 6, 5, 7, 8, 7, 4, 3, 2, 4, 3, 2, 1, 5, 3, 2, 1, 2, 2, 2],\n",
    "#     \"precipitation\": [0.00, 0.00, 0.00, 0.00, 0.30, 0.70, 0.60, 0.80, 1.00, 0.80, 0.30, 0.10,\n",
    "#                       0.00, 0.20, 0.20, 0.00, 0.00, 0.40, 0.20, 0.00, 0.00, 0.10, 0.10, 0.10]\n",
    "# }\n",
    "\n",
    "limit = 100\n",
    "\n",
    "\n",
    "locs = []\n",
    "for loc in data['to'].head(limit):\n",
    "    locs.append(loc)\n",
    "\n",
    "dates = []\n",
    "for time in data['scheduled_time'].head(limit):\n",
    "    dates.append(time[:10])\n",
    "\n",
    "hours = []\n",
    "for time in data['scheduled_time'].head(limit):\n",
    "    hours.append(time[11:13])\n",
    "\n",
    "\n",
    "precipitations = []\n",
    "\n",
    "for i in range(0, limit):\n",
    "    loc = locs[i]\n",
    "    if loc in stations:\n",
    "        lat = stations[loc][0]\n",
    "        lon = stations[loc][1]\n",
    "        hour = int(hours[i])\n",
    "        date = dates[i]\n",
    "        #print(\"Getting weather data for \" + loc + \" on \" + time)\n",
    "        prec = get_weather_data(lat, lon, date, date)\n",
    "        prec = prec[hour]\n",
    "        prec = prec[1]\n",
    "        precipitations.append(prec)\n",
    "        #print(prec)\n",
    "\n",
    "print(precipitations)\n",
    "\n",
    "data = data.drop(columns=['to', 'scheduled_time'])\n",
    "\n",
    "data = data.head(limit)\n",
    "\n",
    "data['precipitation'] = precipitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea620518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vishnu/miniconda3/envs/tf/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 246ms/step - loss: 0.1772 - val_loss: 0.0588\n",
      "Epoch 2/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1560 - val_loss: 0.0524\n",
      "Epoch 3/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1485 - val_loss: 0.0469\n",
      "Epoch 4/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1496 - val_loss: 0.0418\n",
      "Epoch 5/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.1350 - val_loss: 0.0373\n",
      "Epoch 6/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1283 - val_loss: 0.0332\n",
      "Epoch 7/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1333 - val_loss: 0.0294\n",
      "Epoch 8/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1114 - val_loss: 0.0259\n",
      "Epoch 9/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1005 - val_loss: 0.0230\n",
      "Epoch 10/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.1093 - val_loss: 0.0205\n",
      "Epoch 11/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0953 - val_loss: 0.0186\n",
      "Epoch 12/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0968 - val_loss: 0.0170\n",
      "Epoch 13/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0881 - val_loss: 0.0159\n",
      "Epoch 14/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0780 - val_loss: 0.0154\n",
      "Epoch 15/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0648 - val_loss: 0.0154\n",
      "Epoch 16/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0637 - val_loss: 0.0158\n",
      "Epoch 17/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0551 - val_loss: 0.0165\n",
      "Epoch 18/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0466 - val_loss: 0.0175\n",
      "Epoch 19/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0447 - val_loss: 0.0188\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
      "Mean Squared Error on Test Data: 1.0118860918421728\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(df)\n",
    "sequence_length = 5\n",
    "X, y = [], []\n",
    "\n",
    "for i in range(sequence_length, len(data_scaled)):\n",
    "    X.append(data_scaled[i-sequence_length:i])  \n",
    "    y.append(data_scaled[i, 0])  \n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(1)) \n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_test_rescaled = scaler.inverse_transform(np.concatenate([y_test.reshape(-1, 1), np.zeros((y_test.shape[0], 1))], axis=1))[:, 0]\n",
    "y_pred_rescaled = scaler.inverse_transform(np.concatenate([y_pred, np.zeros((y_pred.shape[0], 1))], axis=1))[:, 0]\n",
    "mse = mean_squared_error(y_test_rescaled, y_pred_rescaled)\n",
    "print(\"Mean Squared Error on Test Data:\", mse)\n",
    "\n",
    "model.save('model.keras')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1a55cb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New input sequences shape: (1, 5, 2)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
      "Precipitation: 0.5, Predicted Delay: 2.802623227238655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vishnu/miniconda3/envs/tf/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loadedModel = keras.models.load_model('model.keras')\n",
    "\n",
    "new_precipitation_data = [0.5]  # DEMO NEW PERCIP VALUE\n",
    "\n",
    "new_precipitation_scaled = scaler.transform(np.column_stack((np.zeros(len(new_precipitation_data)), new_precipitation_data)))\n",
    "new_input_sequences = []\n",
    "last_sequence = data_scaled[-sequence_length:].reshape((1, sequence_length, data_scaled.shape[1]))\n",
    "\n",
    "for precip in new_precipitation_scaled:\n",
    "    next_sequence = np.append(last_sequence[:, 1:, :], precip.reshape(1, 1, -1), axis=1)\n",
    "    new_input_sequences.append(next_sequence)\n",
    "\n",
    "new_input_sequences = np.array(new_input_sequences)\n",
    "new_input_sequences = new_input_sequences.reshape((1, 5, 2))  \n",
    "print(\"New input sequences shape:\", new_input_sequences.shape)  \n",
    "new_predictions = loadedModel.predict(new_input_sequences)\n",
    "new_predictions_rescaled = scaler.inverse_transform(np.concatenate([new_predictions, np.zeros((new_predictions.shape[0], 1))], axis=1))[:, 0]\n",
    "\n",
    "for precip, pred in zip(new_precipitation_data, new_predictions_rescaled):\n",
    "    print(f\"Precipitation: {precip}, Predicted Delay: {pred}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
